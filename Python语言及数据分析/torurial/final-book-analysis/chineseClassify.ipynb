{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文文本分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.905 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文语料分词结束！！！\n",
      "中文语料分词结束！！！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jieba\n",
    "\n",
    "# 保存至文件\n",
    "def savefile(savepath, content):\n",
    "    fp = open(savepath, \"w\", encoding=\"utf-8\")\n",
    "    fp.write(content)\n",
    "\n",
    "# 读取文件\n",
    "def readfile(path):\n",
    "    fp = open(path, \"r\", encoding=\"utf-8\")\n",
    "    content = fp.read()\n",
    "    return content\n",
    "\n",
    "def corpus_segment(corpus_path, seg_path):\n",
    "    # 获取corpus_path下的所有子目录\n",
    "    catelist = os.listdir(corpus_path) \n",
    "    # 获取每个目录（类别）下所有的文件\n",
    "    for mydir in catelist:\n",
    "        # 拼出分类子目录的路径如：train_corpus/art/\n",
    "        class_path = corpus_path + mydir + \"/\"  \n",
    "        # 拼出分词后存贮的对应目录路径如：train_corpus_seg/art/\n",
    "        seg_dir = seg_path + mydir + \"/\"  \n",
    " \n",
    "        if not os.path.exists(seg_dir):  \n",
    "            # 是否存在分词目录，如果没有则创建该目录\n",
    "            os.makedirs(seg_dir)\n",
    " \n",
    "        # 获取未分词语料库中某一类别中的所有文本\n",
    "        file_list = os.listdir(class_path)  \n",
    "        for file_path in file_list:  \n",
    "            # 遍历类别目录下的所有文件\n",
    "            fullname = class_path + file_path\n",
    "            content = readfile(fullname)\n",
    "            content = content.replace(\"[\\n0-9]\", \"\")\n",
    "            content = content.replace(\" \", \"\")\n",
    "            content_seg = jieba.cut(content)\n",
    "            savefile(seg_dir + file_path, \" \".join(content_seg))\n",
    "    print (\"中文语料分词结束！！！\")\n",
    " \n",
    "if __name__==\"__main__\":\n",
    "    #对训练集进行分词\n",
    "    # 未分词分类语料库路径\n",
    "    corpus_path = \"./train_corpus/\"  \n",
    "    # 分词后分类语料库路径\n",
    "    seg_path = \"./train_corpus_seg/\"  \n",
    "    corpus_segment(corpus_path,seg_path)\n",
    "\n",
    "    #对测试集进行分词\n",
    "    # 未分词分类语料库路径\n",
    "    corpus_path = \"./test_corpus/\"  \n",
    "    # 分词后分类语料库路径\n",
    "    seg_path = \"./test_corpus_seg/\"  \n",
    "    corpus_segment(corpus_path,seg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化为Bunch类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建文本对象结束！！！\n",
      "构建文本对象结束！！！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.utils import Bunch\n",
    " \n",
    "def _readfile(path):\n",
    "    with open(path, \"r\", encoding=\"utf8\") as fp:\n",
    "        #with as句法前面的代码已经多次介绍过，今后不再注释\n",
    "        content = fp.read()\n",
    "    return content\n",
    " \n",
    "def corpus2Bunch(wordbag_path,seg_path):\n",
    "    # 获取seg_path下的所有子目录，也就是分类信息\n",
    "    catelist = os.listdir(seg_path)\n",
    "    #创建一个Bunch实例\n",
    "    bunch = Bunch(target_name=[], label=[], filenames=[], contents=[])\n",
    "    bunch.target_name.extend(catelist)\n",
    "\n",
    "    # 获取每个目录下所有的文件\n",
    "    for mydir in catelist:\n",
    "        class_path = seg_path + mydir + \"/\"  \n",
    "        file_list = os.listdir(class_path)  \n",
    "        for file_path in file_list:  \n",
    "            fullname = class_path + file_path  \n",
    "            bunch.label.append(mydir)\n",
    "            bunch.filenames.append(fullname)\n",
    "            bunch.contents.append(_readfile(fullname)) \n",
    "    with open(wordbag_path, \"wb\") as file_obj:\n",
    "        pickle.dump(bunch, file_obj)\n",
    "    print(\"构建文本对象结束！！！\")\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    #对训练集进行Bunch化操作：\n",
    "    wordbag_path = \"train_word_bag/train_set.dat\"\n",
    "    seg_path = \"train_corpus_seg/\" \n",
    "    corpus2Bunch(wordbag_path, seg_path)\n",
    " \n",
    "    # 对测试集进行Bunch化操作：\n",
    "    wordbag_path = \"test_word_bag/test_set.dat\"\n",
    "    seg_path = \"test_corpus_seg/\" \n",
    "    corpus2Bunch(wordbag_path, seg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 权重策略--TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if-idf词向量空间实例创建成功\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v:\\Environment\\Annaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# 引入Bunch类\n",
    "from sklearn.utils import Bunch\n",
    "import pickle#之前已经说过，不再赘述\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "# 读取文件\n",
    "def _readfile(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "        content = fp.read()\n",
    "    return content\n",
    " \n",
    "# 读取bunch对象\n",
    "def _readbunchobj(path):\n",
    "    with open(path, \"rb\") as file_obj:\n",
    "        bunch = pickle.load(file_obj)\n",
    "    return bunch\n",
    " \n",
    "# 写入bunch对象\n",
    "def _writebunchobj(path, bunchobj):\n",
    "    with open(path, \"wb\") as file_obj:\n",
    "        pickle.dump(bunchobj, file_obj)\n",
    " \n",
    "#这个函数用于创建TF-IDF词向量空间\n",
    "def vector_space(stopword_path,bunch_path,space_path):\n",
    " \n",
    "    # 读取停用词\n",
    "    stpwrdlst = _readfile(stopword_path).splitlines()\n",
    "    #导入分词后的词向量bunch对象\n",
    "    bunch = _readbunchobj(bunch_path)\n",
    "\n",
    "    tfidfspace = Bunch(target_name=bunch.target_name, label=bunch.label, filenames=bunch.filenames, tdm=[], vocabulary={})\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5)\n",
    "    \n",
    "    #此时tdm里面存储的就是if-idf权值矩阵\n",
    "    tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "    tfidfspace.vocabulary = vectorizer.vocabulary_\n",
    " \n",
    "    _writebunchobj(space_path, tfidfspace)\n",
    "    print(\"if-idf词向量空间实例创建成功\")\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    # 停用词表的路径\n",
    "    stopword_path = \"train_word_bag/hlt_stop_words.txt\"\n",
    "    # 导入训练集Bunch的路径\n",
    "    bunch_path = \"train_word_bag/train_set.dat\"  \n",
    "    # 词向量空间保存路径\n",
    "    space_path = \"train_word_bag/tfdifspace.dat\"  \n",
    "    vector_space(stopword_path,bunch_path,space_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if-idf词向量空间实例创建成功！！！\n"
     ]
    }
   ],
   "source": [
    "# 引入Bunch类\n",
    "from sklearn.utils import Bunch\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "def _readfile(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "        content = fp.read()\n",
    "    return content\n",
    " \n",
    "def _readbunchobj(path):\n",
    "    with open(path, \"rb\") as file_obj:\n",
    "        bunch = pickle.load(file_obj)\n",
    "    return bunch\n",
    " \n",
    "def _writebunchobj(path, bunchobj):\n",
    "    with open(path, \"wb\") as file_obj:\n",
    "        pickle.dump(bunchobj, file_obj)\n",
    " \n",
    "def vector_space(stopword_path,bunch_path,space_path,train_tfidf_path):\n",
    " \n",
    "    stpwrdlst = _readfile(stopword_path).splitlines()\n",
    "    bunch = _readbunchobj(bunch_path)\n",
    "    tfidfspace = Bunch(target_name=bunch.target_name, label=bunch.label, filenames=bunch.filenames, tdm=[], vocabulary={})\n",
    " \n",
    "    #导入训练集的TF-IDF词向量空间\n",
    "    trainbunch = _readbunchobj(train_tfidf_path)\n",
    "    tfidfspace.vocabulary = trainbunch.vocabulary\n",
    " \n",
    "    vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5,vocabulary=trainbunch.vocabulary)\n",
    "    tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "    _writebunchobj(space_path, tfidfspace)\n",
    "    print (\"if-idf词向量空间实例创建成功！！！\")\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    # 停用词表的路径\n",
    "    stopword_path = \"train_word_bag/hlt_stop_words.txt\"\n",
    "    # 词向量空间保存路径\n",
    "    bunch_path = \"test_word_bag/test_set.dat\"   \n",
    "    # TF-IDF词向量空间保存路径\n",
    "    space_path = \"test_word_bag/testspace.dat\"   \n",
    "    train_tfidf_path=\"train_word_bag/tfdifspace.dat\"\n",
    "    vector_space(stopword_path,bunch_path,space_path,train_tfidf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if-idf词向量空间实例创建成功！！！\n",
      "if-idf词向量空间实例创建成功！！！\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import Bunch\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "def _readfile(path):\n",
    "    with open(path, \"rb\") as fp:\n",
    "        content = fp.read()\n",
    "    return content\n",
    " \n",
    "def _readbunchobj(path):\n",
    "    with open(path, \"rb\") as file_obj:\n",
    "        bunch = pickle.load(file_obj)\n",
    "    return bunch\n",
    " \n",
    "def _writebunchobj(path, bunchobj):\n",
    "    with open(path, \"wb\") as file_obj:\n",
    "        pickle.dump(bunchobj, file_obj)\n",
    " \n",
    "def vector_space(stopword_path,bunch_path,space_path,train_tfidf_path=None):\n",
    " \n",
    "    stpwrdlst = _readfile(stopword_path).splitlines()\n",
    "    bunch = _readbunchobj(bunch_path)\n",
    "    tfidfspace = Bunch(target_name=bunch.target_name, label=bunch.label, filenames=bunch.filenames, tdm=[], vocabulary={})\n",
    " \n",
    "    if train_tfidf_path is not None:\n",
    "        trainbunch = _readbunchobj(train_tfidf_path)\n",
    "        tfidfspace.vocabulary = trainbunch.vocabulary\n",
    "        vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5,vocabulary=trainbunch.vocabulary)\n",
    "        tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    " \n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5)\n",
    "        tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "        tfidfspace.vocabulary = vectorizer.vocabulary_\n",
    " \n",
    "    _writebunchobj(space_path, tfidfspace)\n",
    "    print (\"if-idf词向量空间实例创建成功！！！\")\n",
    " \n",
    "if __name__ == '__main__':\n",
    " \n",
    "    stopword_path = \"train_word_bag/hlt_stop_words.txt\"\n",
    "    bunch_path = \"train_word_bag/train_set.dat\"\n",
    "    space_path = \"train_word_bag/tfdifspace.dat\"\n",
    "    vector_space(stopword_path,bunch_path,space_path)\n",
    " \n",
    "    bunch_path = \"test_word_bag/test_set.dat\"\n",
    "    space_path = \"test_word_bag/testspace.dat\"\n",
    "    train_tfidf_path=\"train_word_bag/tfdifspace.dat\"\n",
    "    vector_space(stopword_path,bunch_path,space_path,train_tfidf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_corpus_seg/biography/2.txt : 实际类别: biography  -->预测类别: novels\n",
      "test_corpus_seg/literature/1.txt : 实际类别: literature  -->预测类别: novels\n",
      "test_corpus_seg/success/1.txt : 实际类别: success  -->预测类别: psychology\n",
      "test_corpus_seg/success/2.txt : 实际类别: success  -->预测类别: child\n",
      "预测完毕!!!\n",
      "精度:0.696\n",
      "召回:0.714\n",
      "f1-score:0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v:\\Environment\\Annaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB  # 导入多项式贝叶斯算法\n",
    "\n",
    "# 读取bunch对象\n",
    "def _readbunchobj(path):\n",
    "    with open(path, \"rb\") as file_obj:\n",
    "        bunch = pickle.load(file_obj)\n",
    "    return bunch\n",
    "\n",
    "# 导入训练集\n",
    "trainpath = \"train_word_bag/tfdifspace.dat\"\n",
    "train_set = _readbunchobj(trainpath)\n",
    " \n",
    "# 导入测试集\n",
    "testpath = \"test_word_bag/testspace.dat\"\n",
    "test_set = _readbunchobj(testpath)\n",
    "\n",
    "# 训练分类器：输入词袋向量和分类标签，alpha:0.001 alpha越小，迭代次数越多，精度越高\n",
    "clf = MultinomialNB(alpha=0.001).fit(train_set.tdm, train_set.label)\n",
    "\n",
    "# 预测分类结果\n",
    "predicted = clf.predict(test_set.tdm)\n",
    "\n",
    "for flabel,file_name,expct_cate in zip(test_set.label,test_set.filenames,predicted):\n",
    "    if flabel != expct_cate:\n",
    "        print (file_name,\": 实际类别:\",flabel,\" -->预测类别:\",expct_cate)\n",
    " \n",
    "print (\"预测完毕!!!\")\n",
    "\n",
    "# 计算分类精度：\n",
    "from sklearn import metrics\n",
    "def metrics_result(actual, predict):\n",
    "    print ('精度:{0:.3f}'.format(metrics.precision_score(actual, predict,average='weighted')))\n",
    "    print ('召回:{0:0.3f}'.format(metrics.recall_score(actual, predict,average='weighted')))\n",
    "    print ('f1-score:{0:.3f}'.format(metrics.f1_score(actual, predict,average='weighted')))\n",
    " \n",
    "metrics_result(test_set.label, predicted)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "344437c0be3283926a8297f965ec43df1c136f942b85e9e735910d59a6983e30"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
